# Multi-Agent GRPO â€“ PubMedQA Fine-Tuning Quick Start Guide

A hierarchical multi-agent reinforcement-learning pipeline that fine-tunes a lightweight language model (Qwen 0.5 B) for biomedical yes/no/maybe question answering on **PubMedQA**, using **Group Relative Policy Optimization (GRPO)**.

---

## ðŸŽ¯ What This Code Does

- ðŸ§© Implements a **Managerâ€“Specialist architecture**  
  Manager decides which sub-agent to call â†’ Specialists (`problem_understanding`, `reasoning`, `computation`, `answering`) produce text.

- ðŸ§  **GRPO training**: Managerâ€™s policy/value heads are optimized from reward signals (correctness + efficiency + quality).

- âš•ï¸ **Biomedical focus**: Agents reason over PubMed abstracts to output *yes/no/maybe.*

- ðŸ“Š **Evaluation utilities**: Compare Baseline (random Manager), Trained (GRPO-fine-tuned), and Fixed Pipeline (deterministic 4-step).

---

## âœ… Repository Structure & File Roles

| File | Role |
|------|------|
| `main_grpo.py` | Entry script to start GRPO training (loads dataset â†’ initialize Manager + Specialists â†’ train â†’ save). |
| `manager_agent.py` | Defines `ManagerAgent` (policy/value heads + decision logic) and `FixedSpecialistAgent` (wrapper for LLM generation). |
| `subagents.py` | Prompt templates for each Specialist (PubMedQA-specific instructions + short output constraints). |
| `reward.py` | Reward computation (correctness + efficiency + format quality dense shaping). |
| `grpo_trainer.py` | Implements GRPO update loop (policy loss + value loss + entropy bonus). |
| `utils.py` | Helper class `LocalHF` for local HF model loading/generation (`temperature`, `max_tokens`, device setup). |
| `evaluate_manager_inference_final.py` | Evaluation script â†’ Baseline vs Trained accuracy, saves JSONL logs. |
| `evaluate_manager_inference_final_with_fixed.py` | Adds Fixed Pipeline baseline for three-way comparison. |
| `test_indices_grpo.json` | Validation/test subset indices used after training. |

---

## âš™ï¸ System Requirements

| Resource | Recommended |
|-----------|--------------|
| GPU | RTX 3090/4090 (â‰¥24 GB VRAM) |
| CPU RAM | â‰¥ 32 GB |
| Disk | â‰¥ 10 GB free |
| Python | 3.10 + PyTorch â‰¥ 2.1 |

### Install Dependencies
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers datasets numpy tqdm
```

---

## ðŸ“¦ Dataset Preparation

Dataset (JSON) format:

```json
[
  {
    "question": "Does aspirin reduce stroke risk?",
    "context": "Randomized trials suggest low-dose aspirin lowers ischemic events...",
    "ground_truth": "yes"
  },
  {
    "question": "Is vitamin D useful for fractures?",
    "context": "Meta-analysis shows limited efficacy of vitamin D supplementation...",
    "ground_truth": "no"
  }
]
```

Save as:  
`data/golden_dataset_pubmedqa_qwen2.5_pro_test_500.json`

---

## ðŸš€ Training Workflow

### 1ï¸âƒ£ Run Training
```bash
python main_grpo.py
```

It will:
1. Load dataset & Qwen 0.5 B backbone  
2. Initialize Manager + Specialists  
3. Run GRPO optimization (Epoch Ã— Steps)  
4. Save fine-tuned Manager â†’ `./trained_manager_grpo/manager/`

**Outputs**
```
Epoch 3 Summary:
  Avg Reward: 0.87
  Avg Loss: 1.96
âœ“ Manager saved to ./trained_manager_grpo
âœ“ Validation indices saved to test_indices_grpo.json
```

---

## ðŸ§ª Evaluation Workflow

### 2ï¸âƒ£ Run Validation (2-way comparison)
```bash
python evaluate_manager_inference_final.py
```
Produces:
```
Baseline Manager (Qwen 0.5B): 27.00%
Trained Manager (GRPO): 42.00%
Accuracy Gain: +15.00%
Logs â†’ ./eval_logs/
```

### 3ï¸âƒ£ Run 3-way Evaluation (including Fixed Pipeline)
```bash
python evaluate_manager_inference_final_with_fixed.py
```
Example output:
```
Baseline (random): 27%
Trained (GRPO): 42%
Fixed (sequential): 34%
```

---

## ðŸ”§ Key Hyperparameters

| Parameter | Location | Recommended Value | Notes |
|------------|-----------|-------------------|-------|
| `temperature` | `utils.LocalHF` | 0.7â€“0.8 (train), 0.3â€“0.5 (test) | High T â†’ exploration ; Low T â†’ deterministic evaluation |
| `max_steps` | `main_grpo.py` / eval scripts | 5 | Max Manager decision depth |
| `learning_rate` | `grpo_trainer.py` | 5e-6â€“3e-6 | Lower for stability |
| `ent_coef` | `grpo_trainer.py` | 0.01â€“0.02 | Higher â†’ more exploration |
| `freeze_manager_backbone` | `manager_agent.py` | True | Only train policy/value heads |

---

## ðŸ”¬ Temperature Notes (Important)

- **Training T â‰ˆ 0.8:** Encourages exploration â†’ better reward learning  
- **Inference T â‰ˆ 0.3â€“0.5:** Measures average ability under mild stochasticity  
- **T = 0.1** may cause *policy collapse* (distribution shift / entropy collapse)

---

## ðŸ“Š Expected Performance

| Setting | Accuracy (100 val samples) | Behavior |
|----------|----------------------------|-----------|
| Baseline Manager (random) | 25â€“30 % | Random routing |
| Fixed Pipeline (sequential) | 33â€“40 % | Hard-coded 4-step flow |
| Trained Manager (GRPO) | 40â€“45 % (typical) | Learned routing & structured answers |

---

## ðŸ§© File Outputs

| File / Folder | Content |
|----------------|----------|
| `trained_manager_grpo/manager/heads.pt` | Saved policy / value head weights |
| `test_indices_grpo.json` | Validation set indices |
| `eval_logs/baseline_trajectories.jsonl` | Per-sample baseline trajectories |
| `eval_logs/trained_trajectories.jsonl` | Per-sample trained trajectories |
| `eval_logs/fixed_pipeline_trajectories.jsonl` | Fixed-order runs |
| `summary_*.txt` | Accuracy summaries |

---

## ðŸ’¡ Tips & Best Practices

- Start with 100 samples to verify pipeline; then scale to 500â€“1000.  
- Monitor reward â†’ should rise steadily (0 â†’ 1 +).  
- Use `temperature annealing` (high â†’ low across epochs) to balance exploration/exploitation.  
- Use `--deterministic` flag (or set seeds) for reproducible runs.  
- Use stochastic mode to measure *realistic capability under sampling*.  

---

## âš ï¸ Troubleshooting

| Issue | Likely Cause | Fix |
|--------|--------------|-----|
| `RuntimeError: CUDA out of memory` | Too many tokens / large batch | Lower `max_tokens` or `batch_size` |
| Reward â‰ˆ 0 throughout | Answer format invalid (â€œmaybe yes becauseâ€¦â€) | Check `answering_prompt` â†’ force one-word output |
| Accuracy drops when T â†“ to 0.1 | Distribution shift / entropy collapse | Evaluate with T â‰ˆ 0.4â€“0.8 |
| Trained slower than Fixed | Manager forward adds overhead | Normal; Manager does extra reasoning |
| Baseline too high | Random seed fixed â†’ deterministic behavior | Enable `do_sample=True` for stochastic evaluation |

---

## ðŸ“ˆ Next Steps

1. **Refine Reward Design** (add dense signals for sub-agents).  
2. **Expand Dataset** â†’ 500 + examples for robust training.  
3. **Add Temperature Sweep Eval** â†’ plot accuracy vs T curve.  
4. **Scale Model** â†’ Qwen 1.5 B or 3 B for higher ceiling.  
5. **Experiment with Entropy Regularization & Î»-baseline** for stability.

---

> ðŸ§­ **Summary:**  
> - `main_grpo.py` â†’ train Manager heads via GRPO  
> - `evaluate_manager_inference_final_with_fixed.py` â†’ compare Baseline / Trained / Fixed  
> - Keep training Tâ‰ˆ0.8, test Tâ‰ˆ0.4 for true performance  
> - Expect ~15 pp accuracy gain on PubMedQA with Qwen 0.5 B  

Happy experimenting with your multi-agent GRPO pipeline ðŸš€
